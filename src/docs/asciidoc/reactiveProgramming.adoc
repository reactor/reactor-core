[[intro-reactive]]
= Introduction to Reactive Programming
Reactor is an implementation of the Reactive Programming paradigm, which can be
summed up as:

//TODO find better quote
[quote]
Reactive programming is oriented around data flows and the propagation
of change. This means that the underlying execution model will automatically
propagate changes through the data flow.

In this particular instance, pioneered by the Reactive Extensions (Rx) library
in the .NET ecosystem, and also implemented by RxJava on the JVM, the reactive
aspect is translated in our object-oriented languages to a kind of extension
of the Observer design pattern.

As time went, a standardization emerged through the *Reactive Streams* effort,
a specification which defines a set of interfaces and interaction rules for
reactive libraries on the JVM. It will be integrated into Java 9 (with the
`Flow` class).

One can also compare the main reactive streams pattern with the familiar Iterator
design pattern, as there is a duality to the `Iterable`-`Iterator` pair in all
these libraries. One major difference is that while an Iterator is _pull_ based,
reactive streams are *push*-based.

Using an iterator is quite imperative, even though the method of accessing
values is solely the responsibility of the `Iterable`. Indeed, it is up to the
developer to choose when to access the `next()` item in the sequence. In
reactive streams, the equivalent of the above pair is `Publisher`-`Subscriber`.
But it is the `Publisher` that notifies the Subscriber of newly available values
_as they come_, and this push aspect is key to being reactive. Plus operations
applied to pushed values are expressed declaratively rather than imperatively.

Additionally to pushing values, the error handling and completion aspects are
also covered in a well defined manner, so a `Publisher` can push new values to
its `Subscriber` (calling `onNext`), but also signal an error (calling `onError`
and terminating the sequence) or completion (calling `onComplete` and
terminating the sequence).

[quote]
onNext x 0..N [onError | onComplete]

This approach is very flexible, as the pattern applies indifferently to use
cases where there is at most one value, n values or even an infinite sequence of
values (for instance the ticks of a clock).

But let's step back a bit and reflect on why we would need such an asynchronous
reactive library in the first place.

//TODO flesh out, add more preamble?

== Blocking can be wasteful
Modern applications nowadays can reach huge scales of users, and even though the
capabilities of modern hardware have continued to improve, performance of the
modern software is still a key concern.

There are broadly two ways one can improve a program's performance:

. **parallelize**: use more threads and more hardware resources +
and/or
. **seek more efficiency** in how current resources are used.

Usually, Java developers will naturally write program using blocking code. This
is all well until there is a performance bottleneck, at which point the time
comes to introduce additional thread(s), running similar blocking code. But this
scaling in resource utilization can quickly introduce contention and concurrency
problems.

Worse! If you look closely, as soon as a program involves some latency (notably
I/O, like a database request or a network call), there is a waste of resources
in the sense that the thread now sits idle, waiting for some data.

So the parallelization approach is not a silver bullet: although it is necessary
in order to access the full power of the hardware, it is also complex to reason
about and susceptible to resource wasting...

== Asynchronicity to the rescue?
The second approach described above, seeking more efficiency, can be a solution
to that last problem. By writing _asynchronous_ _non-blocking_ code, you allow
for the execution to switch to another active task **using the same underlying
resources**, and to later come back to the current "train of thought" when the
asynchronous processing has completed.

But how can you produce asynchronous code on the JVM?

Java offers mainly two models of asynchronous programming:

- **Callbacks**: asynchronous methods don't have a return value but take an
extra `callback` parameter (a lambda or simple anonymous class) that will get
called when the result is available. Most well known example is Swing's
`EventListener` hierarchy.
- **Futures**: asynchronous methods return a `Future<T>` **immediately**. The
asynchronous process computes a `T` value, but the future wraps access to it,
isn't immediately valued and can be polled until it becomes valued.
`ExecutorService` running `Callable<T>` tasks use Futures for instance.

So is it good enough? Well, not for every use cases, and both approaches have
limitations...

Callbacks are very hard to compose together, quickly leading to code that is
difficult to read and maintain ("Callback Hell").

Let's take an example: showing top 5 favorites from a user on the UI, or
suggestions if he/she doesn't have any favorite. This goes through 3 services
(one gives favorite IDs, the other fetches favorite details, while the third
offers suggestions with details):

.Example of Callback Hell
[source,java]
----
userService.getFavorites(userId, new Callback<List<String>>() { //<1>
  public void onSuccess(List<String> list) { //<2>
    if (list.isEmpty()) { //<3>
      suggestionService.getSuggestions(new Callback<List<Favorite>>() {
        public void onSuccess(List<Favorite> list) { //<4>
          UiUtils.submitOnUiThread(() -> { //<5>
            list.stream()
                .limit(5)
                .forEach(uiList::show); //<6>
            });
        }

        public void onError(Throwable error) { //<7>
          UiUtils.errorPopup(error);
        }
      });
    } else {
      list.stream() //<8>
          .limit(5)
          .forEach(favId -> favoriteService.getDetails(favId, //<9>
            new Callback<Favorite>() {
              public void onSuccess(Favorite details) {
                UiUtils.submitOnUiThread(() -> uiList.show(details));
              }

              public void onError(Throwable error) {
                UiUtils.errorPopup(error);
              }
            }
          ));
    }
  }

  public void onError(Throwable error) {
    UiUtils.errorPopup(error);
  }
});
----
<1> We have callback-based services: a `Callback` interface with a method invoked
when the async process was successful and one invoked in case of an error.
<2> The first service invokes its callback with the list of favorite IDs.
<3> If the list is empty, we must go to `suggestionService`...
<4> ...and it gives a `List<Favorite>` to a second callback.
<5> Since we're dealing with UI we need to ensure our consuming code will run in
the UI thread.
<6> We use Java 8 `Stream` to limit the number of suggestions processed to 5, and
we show them in a graphical list in the UI.
<7> At each level we'll repeatedly deal with errors the same way: show them in a
popup.
<8> Back to the favorite ID level: if the service returned a full list, then we
need to go to the `favoriteService` to get detailed `Favorite` objects. Since we
only want 5 of them, we first stream the list of IDs to limit it to 5.
<9> Once again, a callback. This time we get a fully-fledged `Favorite` object
that we'll push to the UI inside the UI thread.

That's a lot of code, a bit hard to follow and with repetitive parts.Compare it
with its equivalent in Reactor:

.Example of Reactor code equivalent to callback code
[source,java]
----
userService.getFavorites(userId) // <1>
           .flatMap(favoriteService::getDetails) // <2>
           .switchIfEmpty(suggestionService.getSuggestions()) // <3>
           .take(5) // <4>
           .publishOn(UiUtils.uiThreadScheduler()) // <5>
           .subscribe(uiList::show, UiUtils::errorPopup); // <6>
----
<1> We start with a flow of favorite IDs.
<2> We _asynchronously transform_ these into detailed `Favorite` objects (`flatMap`).
We now have a flow of `Favorite`.
<3> In case the flow of `Favorite` is empty, we switch to a fallback through the
`suggestionService`.
<4> We are only interested in at most 5 elements from the resulting flow.
<5> At the end, we want to process each piece of data in the UI thread.
<6> We trigger the flow by describing what to do with the final form of the data
(show it in a UI list) and what to do in case of an error (show a popup).

What if you wanted to ensure the favorite IDs are retrieved in less than 800ms,
and otherwise get them from a cache? In the callback-based code, that looks like
a complicated task... But in Reactor it becomes as easy as adding a `timeout`
operator in the chain:

.Example of Reactor code with timeout and fallback
[source,java]
----
userService.getFavorites(userId)
           .timeout(Duration.ofMillis(800)) // <1>
           .onErrorResume(cacheService.cachedFavoritesFor(userId)) // <2>
           .flatMap(favoriteService::getDetails) // <3>
           .switchIfEmpty(suggestionService.getSuggestions())
           .take(5)
           .publishOn(UiUtils.uiThreadScheduler())
           .subscribe(uiList::show, UiUtils::errorPopup);
----
<1> If the part above emits nothing for more than 800ms, propagate an error...
<2> ...and in case of any error from above, fallback to the `cacheService`.
<3> The rest of the chain is similar to the original example.

Futures are a bit better, but they are still not so good at composition, despite
the improvements brought in Java 8 by `CompletableFuture`... Orchestrating
multiple futures together is doable, but not that easy. Plus it is very (too?)
easy to stay in familiar territory and block on a `Future` by calling their
`get()` method. And lastly, they lack the support for multiple values and
advanced error handling.

Let's take another example: we get a list of IDs from which we want to fetch a name and
some stat and combine these pair-wise, all of it asynchronously.

.Example of `CompletableFuture` combination
[source,java]
----
CompletableFuture<List<String>> ids = ifhIds(); // <1>

CompletableFuture<List<String>> result = ids.thenComposeAsync(l -> { // <2>
	Stream<CompletableFuture<String>> zip =
			l.stream().map(i -> { // <3>
						 CompletableFuture<String> nameTask = ifhName(i); // <4>
						 CompletableFuture<Integer> statTask = ifhStat(i); // <5>

						 return nameTask.thenCombineAsync(statTask, (name, stat) -> "Name " + name + " has stats " + stat); // <6>
					 });
	List<CompletableFuture<String>> combinationList = zip.collect(Collectors.toList()); // <7>
	CompletableFuture<String>[] combinationArray = combinationList.toArray(new CompletableFuture[combinationList.size()]);

	CompletableFuture<Void> allDone = CompletableFuture.allOf(combinationArray); // <8>
	return allDone.thenApply(v -> combinationList.stream()
												 .map(CompletableFuture::join) // <9>
												 .collect(Collectors.toList()));
});

List<String> results = result.join(); // <10>
assertThat(results).contains(
				"Name NameJoe has stats 103",
				"Name NameBart has stats 104",
				"Name NameHenry has stats 105",
				"Name NameNicole has stats 106",
				"Name NameABSLAJNFOAJNFOANFANSF has stats 121");
----
<1> We start off a future that gives us a list of ids to process.
<2> We want to start some deeper asynchronous processing once we get the list.
<3> For each element in the list...
<4> First we'll asynchronously get the associated name...
<5> Then we'll asynchronously get the associated task...
<6> And we'll combine both results.
<7> We now have a list of futures that represent all the combination tasks.
In order to execute these tasks, we need to convert the list to an array...
<8> ...and pass it to `CompletableFuture.allOf`, which outputs a future that completes
when all tasks have completed.
<9> The tricky bit is that `allOf` returns `CompletableFuture<Void>`, so we reiterate
over the list of futures, collecting their result via `join()` (which here doesn't block
since `allOf` has ensure the futures are all done).
<10> Once the whole asynchronous pipeline has been triggered, we wait for it to be processed
and return the list of results that we can assert.

Since Reactor has more combination operators out of the box, this can be simplified:

.Example of Reactor code equivalent to future code
[source,java]
----
Flux<String> ids = ifhrIds(); // <1>

Flux<String> combinations =
		ids.flatMap(id -> { // <2>
			Mono<String> nameTask = ifhrName(id); // <3>
			Mono<Integer> statTask = ifhrStat(id); // <4>

			return nameTask.and(statTask, // <5>
					(name, stat) -> "Name " + name + " has stats " + stat);
		});

Mono<List<String>> result = combinations.collectList(); // <6>

List<String> results = result.block(); // <7>
assertThat(results).containsExactly( // <8>
		"Name NameJoe has stats 103",
		"Name NameBart has stats 104",
		"Name NameHenry has stats 105",
		"Name NameNicole has stats 106",
		"Name NameABSLAJNFOAJNFOANFANSF has stats 121"
);
----
<1> This time we'll start from an asynchronously provided sequence of ids (a `Flux<String>`)
<2> For each element in the sequence, we'll asynchronously process it (`flatMap`) twice:
<3> First we'll get the associated name
<4> Second we'll get the associated stat
<5> We are actually interested in asynchronously combining these 2 values
<6> Furthermore, we'd like to aggregate the values into a `List` as they become available.
<7> In production, we'd continue working with the `Flux` asynchronously by further combining
it or subscribing to it. Since we're in a test, we'll block waiting for the processing to finish
instead, directly returning the aggregated list of values.
<8> We are now ready to assert the result.

These caveats of Callback and Future seem familiar: aren't they what Reactive Programming directly tries to
address with the `Publisher`-`Subscriber` pair?

== From Imperative to Reactive Programming
Indeed, reactive libraries like Reactor aim at addressing these drawbacks of
"classic" asynchronous approaches on the JVM, while also focusing on a few
additional aspects. To sum it up:

- **Composability** and **readability**
- Data as a **flow** manipulated using a rich vocabulary of **operators**
- Nothing happens until you **subscribe**
- **Backpressure** or _the ability for the consumer to signal the producer that
the rate of emission is too high for it to keep up_
- **High level** but **high value** abstraction that is _concurrency-agnostic_

== Composability and readability
By composability, we mean the ability to orchestrate multiple asynchronous tasks
together, using results from previous tasks to feed input to subsequent ones, or
executing several tasks in a fork-join style, as well as reusing asynchronous
tasks as discrete components in an higher level system.

This is tightly coupled to readability and maintainability of one's code, as
these layers of asynchronous processes get more and more complex. As we saw, the
callback model is simple, but one of its main drawbacks is that for complex
processes you need to have a callback executed from a callback, itself nested
inside another callback, and so on...

That is what is referred to as **Callback Hell**. And as you can guess (or know
from experience), such code is pretty hard to go back to and reason about.

Reactor on the other hand offers rich composition options where code mirrors the
organization of the abstract process, and everything is kept at the same level
(no nesting if it is not necessary).

== The assembly line analogy
You can think of data processed by a reactive application as moving through
an assembly line. Reactor is the conveyor belt and working stations. So the
raw material pours from a source (the original `Publisher`) and ends up as a
finished product ready to be pushed to the consumer (or `Subscriber`).

It can go to various transformations and other intermediary steps, or be part of
a larger assembly line that aggregates intermediate pieces together.

Finally, if there is a glitch or a clogging at one point (for example boxing the
products takes a disproportionately long time), the workstation can signal that
upstream and limit the flow of raw material.

== Operators
In Reactor, operators are what we represented in the above analogy as the
assembly line's workstations. Each operator adds behavior to a `Publisher`, and
it actually wraps the previous step's `Publisher` into a new instance.

The whole chain is thus layered, like an onion, where data originates from the
first `Publisher` in the center and moves outward, transformed by each layer.

TIP: Understanding this can help you avoid a common mistake that would lead you
to believe that an operator you used in your chain is not being applied. See
this <<faq.chain,item>> in the FAQ.

While the Reactive Streams specification doesn't specify operators at all, one
of the high added values of derived reactive libraries like Reactor is the rich
vocabulary of operators that they bring along. These cover a lot of ground, from
simple transformation and filtering to complex orchestration and error handling.

[[reactive.subscribe]]
== Nothing happens until you `subscribe()`
In Reactor when you write a `Publisher` chain, data doesn't start pumping into
it by default. Instead, what you have is a abstract description of your
asynchronous process (which can help with reusability and composition by the
way).

By the act of **subscribing**, you tie the `Publisher` to a `Subscriber`, which
triggers the flow of data in the whole chain. This is achieved internally by a
single `request` signal from the `Subscriber` that is propagated upstream, right
back to the source `Publisher`.

[[reactive.backpressure]]
== Backpressure
The same mechanism is in fact used to implement **backpressure**, which we
described in the assembly line analogy as a feedback signal sent up the line when
a working station is slower to process than the upstream.

The real mechanism defined by the Reactive Streams specification is pretty close
to the analogy: a subscriber can work in _unbounded_ mode and let the source
push all the data at its fastest achievable rate, but can also use the `request`
mechanism to signal the source that it is ready to process at most `n` elements.

Intermediate operators can also change the request in-flight. Imagine a `buffer`
operator that groups elements in batches of 10. If the subscriber requests 1
buffer, then it is acceptable for the source to produce 10 elements. Prefetching
strategies can also be applied is producing the elements before they are
requested is not too costly.

This transforms the push model into a push-pull hybrid where the downstream can
pull n elements from upstream if they are readily available, but if they're not
then they will get pushed by the upstream whenever they are produced.

[[reactive.hotCold]]
== Hot vs Cold
In the Rx family of reactive libraries, one can distinguish two broad categories
of reactive sequences: **hot** and **cold**. This distinction mainly has to do
with how the reactive stream reacts to subscribers:

 - a **Cold** sequence will start anew for each `Subscriber`, including at the
 source of data. If the source wraps an HTTP call, a new HTTP request will be
 made for each subscription
 - a **Hot** sequence will not start from scratch for each `Subscriber`. Rather,
 late subscribers will receive signals emitted _after_ they subscribed. Note
 however that some hot reactive streams can cache or replay the history of
 emissions totally or partially... From a general perspective, a hot sequence
 will emit whether or not there are some subscribers listening.

For more information on hot vs cold in the context of Reactor, see
<<reactor.hotCold,this reactor-specific section>>.

//TODO talk about concurrency agnostic? elements of functional style?
