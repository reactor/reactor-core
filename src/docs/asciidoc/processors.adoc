Processors are a special kind of `Publisher` that are also a `Subscriber`. That
means that you can `subscribe` to a `Processor` (generally, they implement
`Flux`), but you can also call methods to manually inject data into the
sequence or terminate it.

There are several kind of Processors, each with a few particular semantics, but
before you start looking into these, you need to ask yourself the following
question:

= Do I need a Processor?
Most of the time, you should try to avoid using a `Processor`. They are harder
to use correctly and prone to some corner cases.

If you think a `Processor` could be a good match for your use case, ask
yourself if you have tried these two alternatives:

1. Could an operator or combination of operators fit the bill? (See
<<which-operator>>.)
2. Could a <<producing,"generator">> operator work instead? (Generally, these
operators are made to bridge APIs that are not reactive, providing a "sink"
that is similar in concept to a `Processor` in the sense that it lets you
populate the sequence with data or terminate it).

If, after exploring the above alternatives, you still think you need a
`Processor`, read the <<processor-overview>> section below to learn about the different
implementations.

= Safely Produce from Multiple Threads by Using the `Sink` Facade
Rather than directly using Reactor `Processors`, it is a good practice to obtain
a `Sink` for the `Processor` by calling `sink()` once.

`FluxProcessor` sinks safely gate multi-threaded producers and can be used
by applications that generate data from multiple threads concurrently.
For example, a thread-safe serialized sink can be created for
`UnicastProcessor`:

[source,java]
----
UnicastProcessor<Integer> processor = UnicastProcessor.create();
FluxSink<Integer> sink = processor.sink(overflowStrategy);
----

Multiple producer threads may concurrently generate data on this serialized
sink:

[source,java]
----
sink.next(n);
----

Overflow from `next` behaves in two possible ways, depending on the `Processor`
and its configuration:

- An unbounded processor handles the overflow itself by dropping or buffering.
- A bounded processor blocks or "spins" on the `IGNORE` strategy or applies the
`overflowStrategy` behavior specified for the `sink`.


[[processor-overview]]
= Overview of Available Processors
Reactor Core comes with several flavors of `Processor`, which are listed below.
Not all processors have the same semantics, but roughly split into three categories:

 - *direct* (`DirectProcessor` and `UnicastProcessor`): these processors can only push data through
 direct user action (calling their `Sink`'s methods directly).
 - *synchronous* (`EmitterProcessor` and `ReplayProcessor`): these processor can push data both
 through user action or by subscribing to an upstream `Publisher` and synchronously
 draining it.
 - *asynchronous* (`WorkQueueProcessor` and `TopicProcessor`): these processors can push data
 obtained from multiple upstream `Publishers`. They are more heavyweight and are backed by
 a `RingBuffer` data structure in order to deal with their multiple upstreams.

The later are most complex to instantiate, with a lot of different options, so they
expose a `Builder` interface. Simpler processors have static factory methods instead.

== DirectProcessor
A `DirectProcessor` is a processor that can dispatch signals to zero to many `Subscribers`.

It is the simplest one to instantiate, with a single `create()` static factory method.
On the other hand, *it has the limitation of not handling backpressure*.

As a consequence, a `DirectProcessor` will signal an `IllegalStateException` to its
subscribers if you push N elements through it but at least one of its subscribers has
requested less than N.

Once the `Processor` has terminated (usually through its `Sink` `error(Throwable)`
or `complete()` methods being called), it will allow more subscribers to subscribe
but will simply replay the termination signal to them immediately.

== UnicastProcessor
A `UnicastProcessor` can deal with backpressure using an internal buffer. The
trade-off is that it can only have *at most one* `Subscriber`.

A UnicastProcessor has a bit more options, reflected by a few `create` static
factory methods. For instance by default it is _unbounded_: if you push any amount of
data through it while its `Subscriber` hasn't requested yet, it will buffer all of it.

This can be changed by providing a custom `Queue` implementation for the internal
buffering in the `create` factory method. If that queue is bounded, the processor
could reject the push of a value when the buffer is full and not enough request
from downstream has been received.

In that _bounded_ case, the processor can also be built with a callback that is invoked
on each rejected element, allowing for cleanup of these rejected elements.

== EmitterProcessor
An `EmitterProcessor` is capable of emitting to several subscribers, while honoring
backpressure for each of its subscribers. It can also subscribe to a `Publisher`
and relay its signals synchronously.

Initially, when it has no subscriber, it can still accept a few data pushes up to
a configurable `bufferSize`. After that point, if no `Subscriber` has come in and
consumed the data, calls to `onNext` will block until the processor is drained
(which can only happen concurrently by then).

The first `Subscriber` to subscribe will thus receive up to `bufferSize` elements
upon subscribing. However after that the processor stops replaying signals to
additional subscribers. These subsequent subscribers instead only receive the signals
pushed through the processor *after* they've subscribed. The internal buffer is still
used for backpressure purposes.

By default, if all of its subscribers are cancelled (which basically means they've all
un-subscribed), it will clear its internal buffer and stop accepting new subscribers.
This can be tuned by the `autoCancel` parameter in `create` static factory methods.

== ReplayProcessor
A `ReplayProcessor` caches elements that are either pushed directly through its `Sink`
or elements from an upstream `Publisher`, and replays them to late subscribers.

It can be created in multiple configurations: caching a single element (`cacheLast`),
a limited history (`create(int)`), unbounded history (`create()`), time-based replay
window (`createTimeout(Duration)`) or even a combination of history size and time
window (`createSizeOrTimeout(int, Duration)`).

== TopicProcessor
A `TopicProcessor` is an asynchronous processor capable of relaying elements from
multiple upstream `Publishers` when created in the `shared` configuration (see the
`share(boolean)` option of the `builder()`).

Note that the share option is mandatory if you intend on concurrently calling `TopicProcessor`'s
`onNext`, `onComplete` or `onError` methods directly or from concurrent upstream Publisher.

Otherwise, such concurrent calls are illegal, as the processor is then fully compliant
with the Reactive Streams specification.

A `TopicProcessor` is capable of fanning out to multiple `Subscribers`. It does so by
associating a `Thread` to each `Subscriber`, which will run until an `onError` or
`onComplete` signal is pushed through the processor, or until the associated `Subscriber`
is cancelled. The maximum number of downstream subscribers is driven by the `executor`
builder option. Provide a bounded `ExecutorService` to limit it to a specific number.

The processor is backed by a `RingBuffer` data structure that stores pushed signals. Each
`Subscriber` thread keeps track of its associated demand and the correct indexes in
the RingBuffer.

This processor also has an `autoCancel` builder option: if set to `true` (the default), it
will result in the source `Publisher`(s) being cancelled when all subscribers are cancelled.

== WorkQueueProcessor
A `WorkQueueProcessor` is also an asynchronous processor capable of relaying elements from
multiple upstream `Publishers` when created in the `shared` configuration (it shares most
of its builder options with `TopicProcessor`).

It relaxes its compliance with Reactive Streams at the benefit of requiring less resources
than the `TopicProcessor`: it is still based on a `RingBuffer` but avoids the overhead
of creating one consumer `Thread` per `Subscriber`. As a result, it scales better than the
`TopicProcessor`.

The trade-off is that its distribution pattern is a little bit different: requests from
each subscribers all add up together, and the processor relays signals to only one
`Subscriber` at a time, in a kind of round-robin distribution pattern rather than fan-out

NOTE: Note however that a fair round-robin distribution is not guaranteed.

The `WorkQueueProcessor` mostly has the same builder options as the `TopicProcessor`, like
`autoCancel`, `share`, `waitStrategy`... The maximum number of downstream subscribers is
also driven by a configurable `ExecutorService` with the `executor` option.

WARNING: You should take care not to subscribe too many `Subscribers` to a
`WorkQueueProcessor`, as doing so *could lock the processor*. If you need to limit the
number of possible subscribers, preferably do so using a `ThreadPoolExecutor` or a
`ForkJoinPool`: the processor can detect their capacity and throw an exception if you
subscribe one too many times.

//TODO == MonoProcessor
