[[advanced]]
= Advanced features and concepts

== Mutualizing operator usage
From a clean-code perspective, code reuse is generally a good thing. Reactor
offers a few patterns that will help you reuse and mutualize code, notably
for operators or combination of operators that you might want to apply regularly
in your codebase. If you think of a chain of operators as a recipe, you can
create a cookbook of operator recipes.

=== transform
The `transform` operator lets you encapsulate a piece of an operator chain into
a function. That function will be applied to an original operator chain at
assembly time to augment it with the encapsulated operators. Doing so applies
the same operations to all the subscribers of a sequence and is basically
equivalent to chaining the operators directly. Here's an example:

[source,java]
----
Function<Flux<String>, Flux<String>> filterAndMap =
f -> f.filter(color -> !color.equals("orange"))
      .map(String::toUpperCase);

Flux.fromIterable(Arrays.asList("blue", "green", "orange", "purple"))
	.doOnNext(System.out::println)
	.transform(filterAndMap)
	.subscribe(d -> System.out.println("Subscriber to Transformed MapAndFilter: "+d));
----
image::https://raw.githubusercontent.com/reactor/reactor-core/v3.0.7.RELEASE/src/docs/marble/gs-transform.png[Transform Operator : encapsulate flows]

This produces:

----
blue
Subscriber to Transformed MapAndFilter: BLUE
green
Subscriber to Transformed MapAndFilter: GREEN
orange
purple
Subscriber to Transformed MapAndFilter: PURPLE
----

=== compose
The `compose` operator is very similar to `transform` and also lets you
encapsulate operators in a function. The major difference is that this function
is applied to the original sequence *on a per-subscriber basis*. It means that
the function can actually produce a different operator chain for each
subscription (by maintaining some state). Here's an example:

[source,java]
----
AtomicInteger ai = new AtomicInteger();
Function<Flux<String>, Flux<String>> filterAndMap = f -> {
	if (ai.incrementAndGet() == 1) {
return f.filter(color -> !color.equals("orange"))
        .map(String::toUpperCase);
	}
	return f.filter(color -> !color.equals("purple"))
	        .map(String::toUpperCase);
};

Flux<String> composedFlux =
Flux.fromIterable(Arrays.asList("blue", "green", "orange", "purple"))
    .doOnNext(System.out::println)
    .compose(filterAndMap);

composedFlux.subscribe(d -> System.out.println("Subscriber 1 to Composed MapAndFilter :"+d));
composedFlux.subscribe(d -> System.out.println("Subscriber 2 to Composed MapAndFilter: "+d));
----
image::https://raw.githubusercontent.com/reactor/reactor-core/v3.0.7.RELEASE/src/docs/marble/gs-compose.png[Compose Operator : Per Subscriber transformation]

This outputs:

----
blue
Subscriber 1 to Composed MapAndFilter :BLUE
green
Subscriber 1 to Composed MapAndFilter :GREEN
orange
purple
Subscriber 1 to Composed MapAndFilter :PURPLE
blue
Subscriber 2 to Composed MapAndFilter: BLUE
green
Subscriber 2 to Composed MapAndFilter: GREEN
orange
Subscriber 2 to Composed MapAndFilter: ORANGE
purple
----

[[reactor.hotCold]]
== Hot vs Cold
So far we have considered that all `Flux` (and `Mono`) are the same: they all
represent an asynchronous sequence of data, and nothing happens before you
subscribe.

Really, though, there are two broad families of publishers: *cold* and *hot*.

The description above applies to the *cold* family of publishers. They generate
data anew for each subscription If no subscription is created, then data never
gets generated.

Think of an HTTP request: each new subscriber will trigger an HTTP call, but no
call is made if no one is interested in the result.

*Hot* publishers, on the other hand, don't depend on any number of
subscribers. They might start publishing data right away and would continue
doing so whenever a new `Subscriber` comes in (in which case said subscriber
would only see new elements emitted _after_ it subscribed). For hot
publishers, _something_ does indeed happen before you subscribe.

One example of the few hot operators in Reactor is `just`: it directly captures
the value at assembly time and will replay it to anybody subscribing to it
later on. To re-use the HTTP call analogy, if the captured data is the result
of an HTTP call then only one network call is made, when instantiating _just_.

To transform `just` into a _cold_ publisher, you can use `defer`. It defers the
HTTP request in our example to subscription time (and would result in a
separate network call for each new subscription).

NOTE: Most other _hot_ publishers in Reactor extend `Processor`.

Contrast these two other examples:

[source,java]
----
Flux<String> source = Flux.fromIterable(Arrays.asList("blue", "green", "orange", "purple"))
                          .doOnNext(System.out::println)
                          .filter(s -> s.startsWith("o"))
                          .map(String::toUpperCase);

source.subscribe(d -> System.out.println("Subscriber 1: "+d));
source.subscribe(d -> System.out.println("Subscriber 2: "+d));
----

This first example produces:

----
blue
green
orange
Subscriber 1: ORANGE
purple
blue
green
orange
Subscriber 2: ORANGE
purple
----

image::https://raw.githubusercontent.com/reactor/reactor-core/v3.0.7.RELEASE/src/docs/marble/gs-cold.png[Replaying behavior]

Both subscribers catch all four colors, because each subscriber causes the
process defined by the operators on the `Flux` to run.

Compare the first example to this second example:

[source,java]
----
UnicastProcessor<String> hotSource = UnicastProcessor.create();

Flux<String> hotFlux = hotSource.publish()
                                .autoConnect()
                                .map(String::toUpperCase);


hotFlux.subscribe(d -> System.out.println("Subscriber 1 to Hot Source: "+d));

hotSource.onNext("blue");
hotSource.onNext("green");

hotFlux.subscribe(d -> System.out.println("Subscriber 2 to Hot Source: "+d));

hotSource.onNext("orange");
hotSource.onNext("purple");
hotSource.onComplete();
----

The second example produces:
----
Subscriber 1 to Hot Source: BLUE
Subscriber 1 to Hot Source: GREEN
Subscriber 1 to Hot Source: ORANGE
Subscriber 2 to Hot Source: ORANGE
Subscriber 1 to Hot Source: PURPLE
Subscriber 2 to Hot Source: PURPLE
----
image::https://raw.githubusercontent.com/reactor/reactor-core/v3.0.7.RELEASE/src/docs/marble/gs-hot.png[Broadcasting a subscription]

Subscriber 1 catches all four colors. Subscriber 2, having been created after
the first two colors were produced, catches only the last two colors. This
difference accounts for the doubling of "ORANGE" and "PURPLE" in the output.
The process described by the operators on this Flux runs regardless of when
subscriptions have been attached.

== Broadcast to multiple subscribers with `ConnectableFlux`
Sometimes, you want to not only defer some processing to the subscription time
of one subscriber, but you might actually want for several of them to
_rendezvous_ and *then* trigger the subscription / data generation.

This is what `ConnectableFlux` is made for. Two main patterns are covered in the
`Flux` API that return a `ConnectableFlux`: `publish` and `replay`.

* `publish` dynamically tries to respect the demand from its various
subscribers, in terms of backpressure, by forwarding these requests to the
source. Most notably, if any subscriber has a pending demand of `0`, publish
will *pause* its requesting to the source.
* `replay` buffers data seen through the first subscription, up to
configurable limits (in time and buffer size). It will replay the data to
subsequent subscribers.

A `ConnectableFlux` offers additional methods to manage subscriptions downstream
vs subscription to the original source. For instance:

* `connect` can be called manually once you've reached enough subscriptions to
the flux. That will trigger the subscription to the upstream source.
* `autoConnect(n)` can do the same job automatically once `n` subscriptions
have been made.
* `refCount(n)` not only automatically tracks incoming subscriptions but also
detects when these subscriptions are cancelled. If not enough subscribers are
tracked, the source is "disconnected", causing a new subscription to the source
later if additional subscribers appear.
* `refCount(int, Duration)` adds a "grace period": once the number of tracked
subscribers becomes too low, it waits for the `Duration` before disconnecting
the source, potentially allowing for enough new subscribers to come in and cross
the connection threshold again.

Consider the following example:

[source,java]
----
Flux<Integer> source = Flux.range(1, 3)
                           .doOnSubscribe(s -> System.out.println("subscribed to source"));

ConnectableFlux<Integer> co = source.publish();

co.subscribe(System.out::println, e -> {}, () -> {});
co.subscribe(System.out::println, e -> {}, () -> {});

System.out.println("done subscribing");
Thread.sleep(500);
System.out.println("will now connect");

co.connect();
----

This code produces:
----
done subscribing
will now connect
subscribed to source
1
1
2
2
3
3
----

With `autoConnect`:

[source,java]
----
Flux<Integer> source = Flux.range(1, 3)
                           .doOnSubscribe(s -> System.out.println("subscribed to source"));

Flux<Integer> autoCo = source.publish().autoConnect(2);

autoCo.subscribe(System.out::println, e -> {}, () -> {});
System.out.println("subscribed first");
Thread.sleep(500);
System.out.println("subscribing second");
autoCo.subscribe(System.out::println, e -> {}, () -> {});
----

Which outputs:
----
subscribed first
subscribing second
subscribed to source
1
1
2
2
3
3
----

== Three Sorts of Batching
When you have lots of elements and you want to separate them into batches, you
have three broad solutions in Reactor: grouping, windowing, and buffering.
These three are conceptually close, because they redistribute a `Flux<T>` into
an aggregate. Grouping and windowing create a `Flux<Flux<T>>`, while buffering
aggregates into `Collection<T>`.

=== Grouping: `Flux<GroupedFlux<T>>`
Grouping is the act of splitting the source `Flux<T>` into multiple batches by
a *key*.

The associated operator is `groupBy`.

Each group is represented as a `GroupedFlux<T>`, which lets you retrieve the
key via its `key()` method.

There is no necessary continuity in the content of the groups. Once a source
element produces a new key, the group for this key is opened and elements that
match the key end up in said group (several groups could be open at the same
time).

This means that groups:

 1. Are always disjoint (a source element belongs to 1 and only 1 group).
 2. Can contain elements from different places in the original sequence.
 3. Are never empty.

[source,java]
----
StepVerifier.create(
	Flux.just(1, 3, 5, 2, 4, 6, 11, 12, 13)
		.groupBy(i -> i % 2 == 0 ? "even" : "odd")
		.concatMap(g -> g.defaultIfEmpty(-1) //if empty groups, show them
				.map(String::valueOf) //map to string
				.startWith(g.key())) //start with the group's key
	)
	.expectNext("odd", "1", "3", "5", "11", "13")
	.expectNext("even", "2", "4", "6", "12")
	.verifyComplete();
----

WARNING: Grouping is best suited for when you have a medium to low number of
groups. The groups must also imperatively be consumed (eg. in a `flatMap`) so
that `groupBy` will continue fetching data from upstream and feeding more
groups. Sometimes these two constraints multiply and lead to hangs, like when
you have a high cardinality and the concurrency of the `flatMap` consuming the
groups is too low.

// We should provide sample code that produces this problem, to illustrate the
// anti-pattern.

=== Windowing: `Flux<Flux<T>>`
Windowing is the act of splitting the source `Flux<T>` into _windows_, by
criteria of size, time, boundary-defining predicates, or boundary-defining
`Publisher`.

The associated operators are `window`, `windowTimeout`, `windowUntil`,
`windowWhile` and `windowWhen`.

A major difference with `groupBy` is that windows are always sequential. No
more than 2 windows can be open at the same time.

They *can* overlap though. For instance, there is a variant with a `maxSize`
and `skip` parameters. The maxSize is the number of elements after which a
window will close, and the skip parameter is the number of elements in the
source after which a new window is opened. So if `maxSize > skip`, a new window
will open before the previous one closes and the 2 windows will overlap.

This example shows overlapping windows:

[source,java]
----
StepVerifier.create(
	Flux.range(1, 10)
		.window(5, 3) //overlapping windows
		.concatMap(g -> g.defaultIfEmpty(-1)) //show empty windows as -1
	)
		.expectNext(1, 2, 3, 4, 5)
		.expectNext(4, 5, 6, 7, 8)
		.expectNext(7, 8, 9, 10)
		.expectNext(10)
		.verifyComplete();
----

NOTE: With the reverse configuration (`maxSize` < `skip`), some elements from
the source would be dropped and not be part of any window.

In the case of predicate-based windowing via `windowUntil` and `windowWhile`,
having subsequent source elements that don't match the predicate can also lead
to _empty windows_, as demonstrated in this example:

[source,java]
----
StepVerifier.create(
	Flux.just(1, 3, 5, 2, 4, 6, 11, 12, 13)
		.windowWhile(i -> i % 2 == 0)
		.concatMap(g -> g.defaultIfEmpty(-1))
	)
		.expectNext(-1, -1, -1) //respectively triggered by odd 1 3 5
		.expectNext(2, 4, 6) // triggered by 11
		.expectNext(12) // triggered by 13
		.expectNext(-1) // empty completion window, would have been omitted if all matched before onComplete
		.verifyComplete();
----

=== Buffering: `Flux<List<T>>`
Buffering is very close to the behavior of windowing, with a twist: instead of
emitting _windows_ (which each are a `Flux<T>`), it emits _buffers_ (which are
`Collection<T>` - by default `List<T>`).

The operators for buffering mirror those for windowing: `buffer`,
`bufferTimeout`, `bufferUntil`, `bufferWhile`, and `bufferWhen`.

Where the corresponding windowing operator would open a window, a buffering
operator would create a new collection and start adding elements to it. Where a
window would close, the buffering operator would emit the collection.

Buffering can also lead to dropping source elements or having overlapping
buffers, as shown here:

[source,java]
----
StepVerifier.create(
	Flux.range(1, 10)
		.buffer(5, 3) //overlapping buffers
	)
		.expectNext(Arrays.asList(1, 2, 3, 4, 5))
		.expectNext(Arrays.asList(4, 5, 6, 7, 8))
		.expectNext(Arrays.asList(7, 8, 9, 10))
		.expectNext(Collections.singletonList(10))
		.verifyComplete();
----

Unlike in windowing, `bufferUntil` and `bufferWhile` don't emit an empty
buffer, as shown here:

[source,java]
----
StepVerifier.create(
	Flux.just(1, 3, 5, 2, 4, 6, 11, 12, 13)
		.bufferWhile(i -> i % 2 == 0)
	)
	.expectNext(Arrays.asList(2, 4, 6)) // triggered by 11
	.expectNext(Collections.singletonList(12)) // triggered by 13
	.verifyComplete();
----

== Parallelize work with `ParallelFlux`

With multi-core architectures being a commodity nowadays, being able to easily
parallelize work is very important. Reactor helps with that by providing a
special type, `ParallelFlux`, that exposes operators that are optimized for
parallelized work.

To obtain a `ParallelFlux`, one can use the `parallel()` operator on any `Flux`.
*This will not by itself parallelize the work* however, but rather will divide
the workload into "rails" (by default as many rails as there are CPU cores).

In order to tell the resulting ParallelFlux where to execute each rail (and
by extension to execute rails in parallel) you have to use `runOn(Scheduler)`.
Note that there is a recommended dedicated Scheduler for parallel work:
`Schedulers.parallel()`.

Compare the next two code examples:

[source,java]
----
Flux.range(1, 10)
    .parallel(2) //<1>
    .subscribe(i -> System.out.println(Thread.currentThread().getName() + " -> " + i));
----
<1> We force a number of rails instead of relying on the number of CPU cores.

with:
[source,java]
----
Flux.range(1, 10)
    .parallel(2)
    .runOn(Schedulers.parallel())
    .subscribe(i -> System.out.println(Thread.currentThread().getName() + " -> " + i));
----

The first code block produces:
----
main -> 1
main -> 2
main -> 3
main -> 4
main -> 5
main -> 6
main -> 7
main -> 8
main -> 9
main -> 10
----

The second correctly parallelizes on two threads, as shown here:
----
parallel-1 -> 1
parallel-2 -> 2
parallel-1 -> 3
parallel-2 -> 4
parallel-1 -> 5
parallel-2 -> 6
parallel-1 -> 7
parallel-1 -> 9
parallel-2 -> 8
parallel-2 -> 10
----

If, once you've processed your sequence in parallel, you want to revert back to a
"normal" `Flux` and apply the rest of the operator chain in a sequential manner,
you can use the `sequential()` method on `ParallelFlux`.

Note that `sequential()` is implicitly applied if you `subscribe` to the ParallelFlux
with a `Subscriber`, but not when using the lambda-based variants of `subscribe`.

Note also that `subscribe(Subscriber<T>)` merges all the rails, while
`subscribe(Consumer<T>)` runs all the rails. If the `subscribe()` method has a
lambda, each lambda is executed as many times as there are rails.

You can also access individual rails or "groups" as a `Flux<GroupedFlux<T>>` via
the `groups()` method and apply additional operators to them via the
`composeGroup()` method.

[[scheduler-factory]]
== Replacing default `Schedulers`
As we've seen in the <<schedulers>> section, Reactor Core comes with several `Scheduler`
implementations. While you can always create new instances via `new*` factory methods,
each `Scheduler` flavor also has a default singleton instance accessible through the
direct factory method (eg. `Schedulers.elastic()` vs `Schedulers.newElastic()`).

These default instances are the ones used by operators that need a `Scheduler` to work,
when you don't explicitly specify one. For example, `Flux#delayElements(Duration)` uses
the `Schedulers.parallel()` instance.

In some cases however, you might need to change these default instances with something
else in a cross-cutting way, without having to make sure every single operator you call
has your specific `Scheduler` as a parameter. This could be for instrumentation purpose
for example: measuring the time every single scheduled task takes by wrapping the real
schedulers. In other words, you might want to *change the default `Schedulers`*.

This is possible through the `Schedulers.Factory` class. By default a `Factory` creates
all the standard `Scheduler` through similarly named methods. Each of these can be
overridden with your custom implementation.

Additionally, the `Factory` exposes one additional customization methods: `decorateExecutorService`.
It is invoked during creation of every single reactor-core `Scheduler` that is backed by
a `ScheduledExecutorService` (even non-default instances, eg. one created by a call to
`Schedulers.newParallel()`).

This allows to tune the `ScheduledExecutorService` to be used: the default one is exposed
as a `Supplier` and, depending on the type of `Scheduler` being configured, you can choose
to entirely bypass that supplier and return your own instance or you can `get()` the
default instance and wrap it.

IMPORTANT: Once you've created a `Factory` that fits your needs, install it via
`Schedulers.setFactory(Factory)`

Finally, there is a last customizable hook in `Schedulers`: `onHandleError`. This hook is
invoked whenever a `Runnable` task submitted to a `Scheduler` throws an `Exception` (note
that if there is an `UncaughtExceptionHandler` set for the `Thread` that ran the task,
both the handler and the hook will be invoked).


[[hooks]]
== Global Hooks
Reactor has another category of configurable callbacks that are invoked by Reactor
operators in various situations. They are all set in the `Hooks` class, and fall into
three categories:

[[hooks-dropping]]
=== Dropping Hooks
These hooks are invoked when the source of an operator doesn't comply to the Reactive
Streams specification. These kind of errors are outside of the normal execution path (ie
they can't be propagated through `onError`).

Typically, a `Publisher` calls `onNext` on the operator despite having already called
`onCompleted` on it previously. +
In that case, the `onNext` value is _dropped_. Same goes for an extraneous `onError`
signal.

The corresponding hooks, `onNextDropped` and `onErrorDropped`, allow you to provide a
global `Consumer` for these drops. You can for example use it to log the drop and cleanup
resources associated with a value if needed (as it will never make it to the rest of
the reactive chain).

Setting the hooks twice in a row is additive: every consumer you provide is invoked.
The hooks can be fully reset to default using `Hooks.resetOn*Dropped()` methods.

[[hooks-internal]]
=== Internal Error Hook
There is one such hook, `onOperatorError`, which is invoked by operators when an unexpected
`Exception` is thrown during the execution of their `onNext`, `onError` and `onComplete`
methods.

Unlike the previous category, this is still within the normal execution path. A typical
example is the `map` operator with a map function that throws an `Exception` (division by
zero for instance). It is still possible at this point to go through the usual channel of
`onError`, and that is what the operator does.

But first, it passes the `Exception` through `onOperatorError`. The hook lets you inspect
the error (and the incriminating value if relevant) and _change_ the `Exception`. Of course
you can also do something on the side like logging and return the original Exception.

Note that the `onOperatorError` hook can be set multiple times: you can provide a `String`
identifier for a particular `BiFunction`, and subsequent calls with different keys
concatenates the functions, which are all executed. On the other hand, reusing the same key
twice allow you to replace a function you previously set.

As a consequence, the default hook behavior can be both fully reset (using
`Hooks.resetOnOperatorError()`) or partially reset for a specific `key` only
(using `Hooks.resetOnOperatorError(String)`).

[[hooks-assembly]]
=== Assembly Hooks
These hooks tie in the lifecycle of operators. They are invoked when a chain of operators
is assembled (ie. instantiated). `onEachOperator` allow you to dynamically change each
operator as it is assembled in the chain, by returning a different `Publisher`.
`onLastOperator` is similar, except it is only invoked on the last operator in the chain
before the `subscribe`.

Like `onOperatorError`, these hooks are cumulative and can be identified with a key. As
such they can be reset partially or totally.

=== Hook Presets
The `Hooks` utility class provides a couple of preset hooks. These are alternatives to the
default behaviors that you can use simply by calling their corresponding method, rather
than coming up with the hook yourself:

 - `onNextDroppedFail()`: `onNextDropped` used to throw a `Exceptions.failWithCancel()`
 exception. It now defaults to logging the dropped value at DEBUG level. To go back to the
 old default behavior of throwing, use `onNextDroppedFail()`.

 - `onOperatorDebug()`: this activates <<debug-activate,debug mode>>. It ties into the
 `onOperatorError` hook, so calling `resetOnOperatorError()` will also reset it. It can be
 independently reset via `resetOnOperatorDebug()` as it uses a specific key internally.

[[context]]
== Adding a Context to a Reactive Sequence
One of the big technical challenges encountered when switching from an imperative
programming perspective to a reactive programming mindset lies in how you deal with threading.

Contrary to what you're used to, in reactive programming a `Thread` can (and will) be used
to process several asynchronous sequences that run roughly at the same time (actually, in
non-blocking locksteps). The execution can also very easily and very often jump from a
thread to another.

This is especially hard for developers that use features dependent on the threading model
being more "stable", like `ThreadLocal`. As it allows you to associate data with a
*thread*, it becomes tricky to use in a reactive context. As a result, libraries that rely
on `ThreadLocal` will at least introduce new challenges when used with Reactor, if not
work badly. Using the MDC of Logback to store and log correlation IDs is a prime example
of such a situation.

The usual workaround for `ThreadLocal` usage is to move the contextual data `C` along your
business data `T` in the sequence, by using `Tuple2<T, C>` for instance. This doesn't look
very good and leaks an orthogonal concern (the contextual data) into your method and `Flux`
signatures.

=== The `Context`
Since `3.1.0`, Reactor comes with an advanced feature that is somewhat comparable to
`ThreadLocal`, but applied to a `Flux` or a `Mono` instead of a `Thread`: the `Context`.

IMPORTANT: This is an advanced feature that is more targeted at library developers. It
requires good understanding of the lifecycle of a `Subscription`, and is intended for
libraries that are responsible for the subscriptions.

A `Context` is an interface reminiscent of `Map`: it stores key-value pairs and allows you
to fetch a value you stored by its key. More specifically:

  * both key and values are `Object`, so a `Context` can contain any number of highly
  different values from different libraries and sources.
  * a `Context` is *immutable*.
  * use `put(Object key, Object value)` to store a key-value pair, returning a new `Context`
  instance. You can also merge two contexts into a new one using `putAll(Context)`.
  * you can check if the key is present with `hasKey(Object key)`.
  * use `getOrDefault(Object key, T defaultValue)` to retrieve a value (cast to a `T`) or
  fall back to a default one if the Context doesn't have that key.
  * use `getOrEmpty(Object key)` to get an `Optional<T>` (the context will attempt to cast
  the stored value to `T`).
  * use `delete(Object key)` to remove the value associated to a key, returning a new
  `Context`.

TIP: *Creating a* `Context`: You can create pre-valued contexts with up to five key-value
pairs using the static `Context.of` methods. It takes 2, 4, 6, 8 or 10 `Object`, each
couple of `Object` being a key-value pair to add to the `Context`. +
 +
Alternatively you can also create a `Context.empty()`.


=== Tying the `Context` to a `Flux`
To make the context useful, it must be tied to a specific sequence and be accessible by
each operator in a chain.footnote:[as long as it is a Reactor native operator, as `Context`
is specific to Reactor]

Actually, a `Context` is tied to each `Subscriber` to a chain. It uses the `Subscription`
propagation mechanism to make itself available to each operator, starting with the final
`subscribe` and moving up the chain.

In order to populate the `Context`, which can only be done at subscription time, you need
to use the `subscriberContext` operator.

Simply use `subscriberContext(Context)`, which merges the Context you provide and the
Context from downstream (remember, the Context is propagated from the bottom of the chain
towards the top). This is done via `putAll`, resulting in a new `Context` for upstream.

TIP: You can also use the more advanced `subscriberContext(Function<Context, Context>)`.
It receives the state of the Context from downstream and lets you put or delete values as
you see fit, returning the new `Context` to use. You can even decide to return a
completely different instance, although it is really not recommended (this might impact
libraries that depend on the `Context`).

=== Using the Context
Populating the `Context` is one aspect, but retrieving that data from it is of course just
as important. Most of the time, the responsibility of putting information into the
`Context` will be on the end user's side, while exploiting that information will be on the
library's side, as the library is usually upstream of the client code.

The tool for reading data from the context is the static `Mono.subscriberContext()` method.

Let's take an example of a library reading information from the `Context`: a reactive
HTTP client that takes a `Mono<String>` as the source of data for a `PUT`, but also looks
for a particular Context key to add a correlation ID to the request's headers.

From the user perspective, it is called like this:

[source,java]
----
doPut("www.example.com", Mono.just("Walter"))
----

In order to propagate a correlation ID, it would be called like that:

[source,java]
----
doPut("www.example.com", Mono.just("Walter"))
	.subscriberContext(Context.of(HTTP_CORRELATION_ID, "2-j3r9afaf92j-afkaf"))
----

As you can see in the snippets above, the user code uses `subscriberContext` to populate
a `Context` with an `HTTP_CORRELATION_ID` key-value pair. The upstream of the operator is
a `Mono<Tuple2<Integer, String>>` (a simplistic representation of an HTTP response)
returned by the HTTP client library. So it is effectively passing information from the
user code to the library code.

Here is the mock code from the library's perspective that reads the context and "augments
the request" if it can find the correlation ID:

[source,java]
----
static final String HTTP_CORRELATION_ID = "reactive.http.library.correlationId";

Mono<Tuple2<Integer, String>> doPut(String url, Mono<String> data) {
	Mono<Tuple2<String, Optional<Object>>> dataAndContext =
			data.zipWith(Mono.subscriberContext() // <1>
			                 .map(c -> c.getOrEmpty(HTTP_CORRELATION_ID))); // <2>

	return dataAndContext
			.<String>handle((dac, sink) -> {
				if (dac.getT2().isPresent()) { // <3>
					sink.next("PUT <" + dac.getT1() + "> sent to " + url + " with header X-Correlation-ID = " + dac.getT2().get());
				}
				else {
					sink.next("PUT <" + dac.getT1() + "> sent to " + url);
				}
				sink.complete();
			})
			.map(msg -> Tuples.of(200, msg));
}
----
<1> Materialize the `Context` through `Mono.subscriberContext()`
<2> Extract value for a the correlation id key, as an `Optional`
<3> If the key was present in the context, use the correlation id as a header

In the library snippet, you can see how it zips the data `Mono` with `Mono.subscriberContext()`.
This gives the library a `Tuple2<String, Context>` and that context will indeed contain
the `HTTP_CORRELATION_ID` entry from downstream (as it is on the direct path to the
subscriber).

The library code then uses `map` to extract an `Optional<String>` for that key, and if the
entry is present it will use the passed correlation id as a `X-Correlation-ID` header.
That last part is simulated by the `handle` above.

The whole test that validates the library code used the correlation ID can be written as:
[source,java]
----
@Test
public void contextForLibraryReactivePut() {
	Mono<String> put = doPut("www.example.com", Mono.just("Walter"))
			.subscriberContext(Context.of(HTTP_CORRELATION_ID, "2-j3r9afaf92j-afkaf"))
			.filter(t -> t.getT1() < 300)
			.map(Tuple2::getT2);

	StepVerifier.create(put)
	            .expectNext("PUT <Walter> sent to www.example.com with header X-Correlation-ID = 2-j3r9afaf92j-afkaf")
	            .verifyComplete();
}
----

[[null-safety]]
== Null-safety

Although Java does not allow to express null-safety with its type system, Reactor
now provides annotations to declare nullability of APIs, similar to those provided by
Spring Framework 5.

Reactor leverages itself these annotations, but they can also be used in any Reactor based
Java project to declare null-safe APIs. Nullability of types used inside method bodies
is outside of the scope of this feature.

These annotations are meta-annotated with https://jcp.org/en/jsr/detail?id=305[JSR 305]
annotations (a dormant JSR but supported by tools like IntelliJ IDEA) to provide useful
warnings to Java developers related to null-safety in order to avoid `NullPointerException`
at runtime. JSR 305 meta-annotations allows tooling vendors to provide null-safety support
in a generic way, without having to hard-code support for Reactor annotations.

[NOTE]
====
It is not necessary with Kotlin 1.1.5+ nor recommanded to have JSR 305 dependency in user project classpath.
====

They are also used by Kotlin which supports natively
https://kotlinlang.org/docs/reference/null-safety.html[null-safety], see
<<kotlin-null-safety,this dedicated section>> for more details.

The following annotations are provided in the `reactor.util.annotation` package:

 * https://projectreactor.io/docs/core/release/api/reactor/util/annotation/NonNull.html[`@NonNull`] annotation where specific parameter,
 return value or field cannot be `null`
 (not needed on parameter and return value where `@NonNullApi` applies) .
 * https://projectreactor.io/docs/core/release/api/reactor/util/annotation/Nullable.html[`@Nullable`] annotation where specific
 parameter, return value or field can be `null`.
 * https://projectreactor.io/docs/core/release/api/reactor/util/annotation/NonNullApi.html[`@NonNullApi`] annotation at package level
 declares non-null as the default behavior for parameters and return values.

[NOTE]
====
Generic type arguments, varargs and array elements nullability are not supported yet,
see https://github.com/reactor/reactor-core/issues/878[issue #878] for up-to-date informations.
====