[[advanced]]
= Advanced features and concepts

== Mutualizing operator usage
From a clean-code perspective, code reuse is generally a good thing. Reactor
offers a few patterns that will help you reuse and mutualize code, notably
for operators or combination of operators that you might want to apply regularly
in your codebase. If you think of a chain of operators as a recipe, you can
create a cookbook of operator recipes.

=== transform
The `transform` operator lets you encapsulate a piece of an operator chain into
a function. That function will be applied to an original operator chain at
assembly time to augment it with the encapsulated operators. Doing so applies
the same operations to all the subscribers of a sequence and is basically
equivalent to chaining the operators directly. Here's an example:

[source,java]
----
Function<Flux<String>, Flux<String>> filterAndMap =
f -> f.filter(color -> !color.equals("orange"))
      .map(String::toUpperCase);

Flux.fromIterable(Arrays.asList("blue", "green", "orange", "purple"))
	.doOnNext(System.out::println)
	.transform(filterAndMap)
	.subscribe(d -> System.out.println("Subscriber to Transformed MapAndFilter: "+d));
----
image::https://raw.githubusercontent.com/reactor/reactor-core/v3.0.7.RELEASE/src/docs/marble/gs-transform.png[Transform Operator : encapsulate flows]

This produces:

----
blue
Subscriber to Transformed MapAndFilter: BLUE
green
Subscriber to Transformed MapAndFilter: GREEN
orange
purple
Subscriber to Transformed MapAndFilter: PURPLE
----

=== compose
The `compose` operator is very similar to `transform` and also lets you
encapsulate operators in a function. The major difference is that this function
is applied to the original sequence *on a per-subscriber basis*. It means that
the function can actually produce a different operator chain for each
subscription (by maintaining some state). Here's an example:

[source,java]
----
AtomicInteger ai = new AtomicInteger();
Function<Flux<String>, Flux<String>> filterAndMap = f -> {
	if (ai.incrementAndGet() == 1) {
return f.filter(color -> !color.equals("orange"))
        .map(String::toUpperCase);
	}
	return f.filter(color -> !color.equals("purple"))
	        .map(String::toUpperCase);
};

Flux<String> composedFlux =
Flux.fromIterable(Arrays.asList("blue", "green", "orange", "purple"))
    .doOnNext(System.out::println)
    .compose(filterAndMap);

composedFlux.subscribe(d -> System.out.println("Subscriber 1 to Composed MapAndFilter :"+d));
composedFlux.subscribe(d -> System.out.println("Subscriber 2 to Composed MapAndFilter: "+d));
----
image::https://raw.githubusercontent.com/reactor/reactor-core/v3.0.7.RELEASE/src/docs/marble/gs-compose.png[Compose Operator : Per Subscriber transformation]

This outputs:

----
blue
Subscriber 1 to Composed MapAndFilter :BLUE
green
Subscriber 1 to Composed MapAndFilter :GREEN
orange
purple
Subscriber 1 to Composed MapAndFilter :PURPLE
blue
Subscriber 2 to Composed MapAndFilter: BLUE
green
Subscriber 2 to Composed MapAndFilter: GREEN
orange
Subscriber 2 to Composed MapAndFilter: ORANGE
purple
----

[[reactor.hotCold]]
== Hot vs Cold
So far we have considered that all `Flux` (and `Mono`) are the same: they all
represent an asynchronous sequence of data, and nothing happens before you
subscribe.

Really, though, there are two broad families of publishers: *cold* and *hot*.

The description above applies to the *cold* family of publishers. They generate
data anew for each subscription If no subscription is created, then data never
gets generated.

Think of an HTTP request: each new subscriber will trigger an HTTP call, but no
call is made if no one is interested in the result.

*Hot* publishers, on the other hand, don't depend on any number of
subscribers. They might start publishing data right away and would continue
doing so whenever a new `Subscriber` comes in (in which case said subscriber
would only see new elements emitted _after_ it subscribed). For hot
publishers, _something_ does indeed happen before you subscribe.

One example of the few hot operators in Reactor is `just`: it directly captures
the value at assembly time and will replay it to anybody subscribing to it
later on. To re-use the HTTP call analogy, if the captured data is the result
of an HTTP call then only one network call is made, when instantiating _just_.

To transform `just` into a _cold_ publisher, you can use `defer`. It defers the
HTTP request in our example to subscription time (and would result in a
separate network call for each new subscription).

NOTE: Most other _hot_ publishers in Reactor extend `Processor`.

Contrast these two other examples:

[source,java]
----
Flux<String> source = Flux.fromIterable(Arrays.asList("blue", "green", "orange", "purple"))
                          .doOnNext(System.out::println)
                          .filter(s -> s.startsWith("o"))
                          .map(String::toUpperCase);

source.subscribe(d -> System.out.println("Subscriber 1: "+d));
source.subscribe(d -> System.out.println("Subscriber 2: "+d));
----

This first example produces:

----
blue
green
orange
Subscriber 1: ORANGE
purple
blue
green
orange
Subscriber 2: ORANGE
purple
----

image::https://raw.githubusercontent.com/reactor/reactor-core/v3.0.7.RELEASE/src/docs/marble/gs-cold.png[Replaying behavior]

Both subscribers catch all four colors, because each subscriber causes the
process defined by the operators on the `Flux` to run.

Compare the first example to this second example:

[source,java]
----
UnicastProcessor<String> hotSource = UnicastProcessor.create();

Flux<String> hotFlux = hotSource.publish()
                                .autoConnect()
                                .map(String::toUpperCase);


hotFlux.subscribe(d -> System.out.println("Subscriber 1 to Hot Source: "+d));

hotSource.onNext("blue");
hotSource.onNext("green");

hotFlux.subscribe(d -> System.out.println("Subscriber 2 to Hot Source: "+d));

hotSource.onNext("orange");
hotSource.onNext("purple");
hotSource.onComplete();
----

The second example produces:
----
Subscriber 1 to Hot Source: BLUE
Subscriber 1 to Hot Source: GREEN
Subscriber 1 to Hot Source: ORANGE
Subscriber 2 to Hot Source: ORANGE
Subscriber 1 to Hot Source: PURPLE
Subscriber 2 to Hot Source: PURPLE
----
image::https://raw.githubusercontent.com/reactor/reactor-core/v3.0.7.RELEASE/src/docs/marble/gs-hot.png[Broadcasting a subscription]

Subscriber 1 catches all four colors. Subscriber 2, having been created after
the first two colors were produced, catches only the last two colors. This
difference accounts for the doubling of "ORANGE" and "PURPLE" in the output.
The process described by the operators on this Flux runs regardless of when
subscriptions have been attached.

== Broadcast to multiple subscribers with `ConnectableFlux`
Sometimes, you want to not only defer some processing to the subscription time
of one subscriber, but you might actually want for several of them to
_rendezvous_ and *then* trigger the subscription / data generation.

This is what `ConnectableFlux` is made for. Two main patterns are covered in the
`Flux` API that return a `ConnectableFlux`: `publish` and `replay`.

* `publish` dynamically tries to respect the demand from its various
subscribers, in terms of backpressure, by forwarding these requests to the
source. Most notably, if any subscriber has a pending demand of `0`, publish
will *pause* its requesting to the source.
* `replay` buffers data seen through the first subscription, up to
configurable limits (in time and buffer size). It will replay the data to
subsequent subscribers.

A `ConnectableFlux` offers additional methods to manage subscriptions downstream
vs subscription to the original source. For instance:

* `connect` can be called manually once you've reached enough subscriptions to
the flux. That will trigger the subscription to the upstream source.
* `autoConnect(n)` can do the same job automatically once `n` subscriptions
have been made.
* `refCount(n)` not only automatically tracks incoming subscriptions but also
detects when these subscriptions are cancelled. If not enough subscribers are
tracked, the source is "disconnected", causing a new subscription to the source
later if additional subscribers appear.
* `refCount(int, Duration)` adds a "grace period": once the number of tracked
subscribers becomes too low, it waits for the `Duration` before disconnecting
the source, potentially allowing for enough new subscribers to come in and cross
the connection threshold again.

Consider the following example:

[source,java]
----
Flux<Integer> source = Flux.range(1, 3)
                           .doOnSubscribe(s -> System.out.println("subscribed to source"));

ConnectableFlux<Integer> co = source.publish();

co.subscribe(System.out::println, e -> {}, () -> {});
co.subscribe(System.out::println, e -> {}, () -> {});

System.out.println("done subscribing");
Thread.sleep(500);
System.out.println("will now connect");

co.connect();
----

This code produces:
----
done subscribing
will now connect
subscribed to source
1
1
2
2
3
3
----

With `autoConnect`:

[source,java]
----
Flux<Integer> source = Flux.range(1, 3)
                           .doOnSubscribe(s -> System.out.println("subscribed to source"));

Flux<Integer> autoCo = source.publish().autoConnect(2);

autoCo.subscribe(System.out::println, e -> {}, () -> {});
System.out.println("subscribed first");
Thread.sleep(500);
System.out.println("subscribing second");
autoCo.subscribe(System.out::println, e -> {}, () -> {});
----

Which outputs:
----
subscribed first
subscribing second
subscribed to source
1
1
2
2
3
3
----

== Three Sorts of Batching
When you have lots of elements and you want to separate them into batches, you
have three broad solutions in Reactor: grouping, windowing, and buffering.
These three are conceptually close, because they redistribute a `Flux<T>` into
an aggregate. Grouping and windowing create a `Flux<Flux<T>>`, while buffering
aggregates into `Collection<T>`.

=== Grouping: `Flux<GroupedFlux<T>>`
Grouping is the act of splitting the source `Flux<T>` into multiple batches by
a *key*.

The associated operator is `groupBy`.

Each group is represented as a `GroupedFlux<T>`, which lets you retrieve the
key via its `key()` method.

There is no necessary continuity in the content of the groups. Once a source
element produces a new key, the group for this key is opened and elements that
match the key end up in said group (several groups could be open at the same
time).

This means that groups:

 1. Are always disjoint (a source element belongs to 1 and only 1 group).
 2. Can contain elements from different places in the original sequence.
 3. Are never empty.

[source,java]
----
StepVerifier.create(
	Flux.just(1, 3, 5, 2, 4, 6, 11, 12, 13)
		.groupBy(i -> i % 2 == 0 ? "even" : "odd")
		.concatMap(g -> g.defaultIfEmpty(-1) //if empty groups, show them
				.map(String::valueOf) //map to string
				.startWith(g.key())) //start with the group's key
	)
	.expectNext("odd", "1", "3", "5", "11", "13")
	.expectNext("even", "2", "4", "6", "12")
	.verifyComplete();
----

WARNING: Grouping is best suited for when you have a medium to low number of
groups. The groups must also imperatively be consumed (eg. in a `flatMap`) so
that `groupBy` will continue fetching data from upstream and feeding more
groups. Sometimes these two constraints multiply and lead to hangs, like when
you have a high cardinality and the concurrency of the `flatMap` consuming the
groups is too low.

// We should provide sample code that produces this problem, to illustrate the
// anti-pattern.

=== Windowing: `Flux<Flux<T>>`
Windowing is the act of splitting the source `Flux<T>` into _windows_, by
criteria of size, time, boundary-defining predicates, or boundary-defining
`Publisher`.

The associated operators are `window`, `windowTimeout`, `windowUntil`,
`windowWhile` and `windowWhen`.

A major difference with `groupBy` is that windows are always sequential. No
more than 2 windows can be open at the same time.

They *can* overlap though. For instance, there is a variant with a `maxSize`
and `skip` parameters. The maxSize is the number of elements after which a
window will close, and the skip parameter is the number of elements in the
source after which a new window is opened. So if `maxSize > skip`, a new window
will open before the previous one closes and the 2 windows will overlap.

This example shows overlapping windows:

[source,java]
----
StepVerifier.create(
	Flux.range(1, 10)
		.window(5, 3) //overlapping windows
		.concatMap(g -> g.defaultIfEmpty(-1)) //show empty windows as -1
	)
		.expectNext(1, 2, 3, 4, 5)
		.expectNext(4, 5, 6, 7, 8)
		.expectNext(7, 8, 9, 10)
		.expectNext(10)
		.verifyComplete();
----

NOTE: With the reverse configuration (`maxSize` < `skip`), some elements from
the source would be dropped and not be part of any window.

In the case of predicate-based windowing via `windowUntil` and `windowWhile`,
having subsequent source elements that don't match the predicate can also lead
to _empty windows_, as demonstrated in this example:

[source,java]
----
StepVerifier.create(
	Flux.just(1, 3, 5, 2, 4, 6, 11, 12, 13)
		.windowWhile(i -> i % 2 == 0)
		.concatMap(g -> g.defaultIfEmpty(-1))
	)
		.expectNext(-1, -1, -1) //respectively triggered by odd 1 3 5
		.expectNext(2, 4, 6) // triggered by 11
		.expectNext(12) // triggered by 13
		.expectNext(-1) // empty completion window, would have been omitted if all matched before onComplete
		.verifyComplete();
----

=== Buffering: `Flux<List<T>>`
Buffering is very close to the behavior of windowing, with a twist: instead of
emitting _windows_ (which each are a `Flux<T>`), it emits _buffers_ (which are
`Collection<T>` - by default `List<T>`).

The operators for buffering mirror those for windowing: `buffer`,
`bufferTimeout`, `bufferUntil`, `bufferWhile`, and `bufferWhen`.

Where the corresponding windowing operator would open a window, a buffering
operator would create a new collection and start adding elements to it. Where a
window would close, the buffering operator would emit the collection.

Buffering can also lead to dropping source elements or having overlapping
buffers, as shown here:

[source,java]
----
StepVerifier.create(
	Flux.range(1, 10)
		.buffer(5, 3) //overlapping buffers
	)
		.expectNext(Arrays.asList(1, 2, 3, 4, 5))
		.expectNext(Arrays.asList(4, 5, 6, 7, 8))
		.expectNext(Arrays.asList(7, 8, 9, 10))
		.expectNext(Collections.singletonList(10))
		.verifyComplete();
----

Unlike in windowing, `bufferUntil` and `bufferWhile` don't emit an empty
buffer, as shown here:

[source,java]
----
StepVerifier.create(
	Flux.just(1, 3, 5, 2, 4, 6, 11, 12, 13)
		.bufferWhile(i -> i % 2 == 0)
	)
	.expectNext(Arrays.asList(2, 4, 6)) // triggered by 11
	.expectNext(Collections.singletonList(12)) // triggered by 13
	.verifyComplete();
----

== Parallelize work with `ParallelFlux`

With multi-core architectures being a commodity nowadays, being able to easily
parallelize work is very important. Reactor helps with that by providing a
special type, `ParallelFlux`, that exposes operators that are optimized for
parallelized work.

To obtain a `ParallelFlux`, one can use the `parallel()` operator on any `Flux`.
*This will not by itself parallelize the work* however, but rather will divide
the workload into "rails" (by default as many rails as there are CPU cores).

In order to tell the resulting ParallelFlux where to execute each rail (and
by extension to execute rails in parallel) you have to use `runOn(Scheduler)`.
Note that there is a recommended dedicated Scheduler for parallel work:
`Schedulers.parallel()`.

Compare the next two code examples:

[source,java]
----
Flux.range(1, 10)
    .parallel(2) //<1>
    .subscribe(i -> System.out.println(Thread.currentThread().getName() + " -> " + i));
----
<1> We force a number of rails instead of relying on the number of CPU cores.

with:
[source,java]
----
Flux.range(1, 10)
    .parallel(2)
    .runOn(Schedulers.parallel())
    .subscribe(i -> System.out.println(Thread.currentThread().getName() + " -> " + i));
----

The first code block produces:
----
main -> 1
main -> 2
main -> 3
main -> 4
main -> 5
main -> 6
main -> 7
main -> 8
main -> 9
main -> 10
----

The second correctly parallelizes on two threads, as shown here:
----
parallel-1 -> 1
parallel-2 -> 2
parallel-1 -> 3
parallel-2 -> 4
parallel-1 -> 5
parallel-2 -> 6
parallel-1 -> 7
parallel-1 -> 9
parallel-2 -> 8
parallel-2 -> 10
----

If, once you've processed your sequence in parallel, you want to revert back to a
"normal" `Flux` and apply the rest of the operator chain in a sequential manner,
you can use the `sequential()` method on `ParallelFlux`.

Note that `sequential()` is implicitly applied if you `subscribe` to the ParallelFlux
with a `Subscriber`, but not when using the lambda-based variants of `subscribe`.

Note also that `subscribe(Subscriber<T>)` merges all the rails, while
`subscribe(Consumer<T>)` runs all the rails. If the `subscribe()` method has a
lambda, each lambda is executed as many times as there are rails.

You can also access individual rails or "groups" as a `Flux<GroupedFlux<T>>` via
the `groups()` method and apply additional operators to them via the
`composeGroup()` method.

[[scheduler-factory]]
== Replacing default `Schedulers`
As we've seen in the <<schedulers>> section, Reactor Core comes with several `Scheduler`
implementations. While you can always create new instances via `new*` factory methods,
each `Scheduler` flavor also has a default singleton instance accessible through the
direct factory method (eg. `Schedulers.elastic()` vs `Schedulers.newElastic()`).

These default instances are the ones used by operators that need a `Scheduler` to work,
when you don't explicitly specify one. For example, `Flux#delayElements(Duration)` uses
the `Schedulers.parallel()` instance.

In some cases however, you might need to change these default instances with something
else in a cross-cutting way, without having to make sure every single operator you call
has your specific `Scheduler` as a parameter. This could be for instrumentation purpose
for example: measuring the time every single scheduled task takes by wrapping the real
schedulers. In other words, you might want to *change the default `Schedulers`*.

This is possible through the `Schedulers.Factory` class. By default a `Factory` creates
all the standard `Scheduler` through similarly named methods. Each of these can be
overridden with your custom implementation.

Additionally, the `Factory` exposes one additional customization methods: `decorateExecutorService`.
It is invoked during creation of every single reactor-core `Scheduler` that is backed by
a `ScheduledExecutorService` (even non-default instances, eg. one created by a call to
`Schedulers.newParallel()`).

This allows to tune the `ScheduledExecutorService` to be used: the default one is exposed
as a `Supplier` and, depending on the type of `Scheduler` being configured, you can choose
to entirely bypass that supplier and return your own instance or you can `get()` the
default instance and wrap it.

IMPORTANT: Once you've created a `Factory` that fits your needs, install it via
`Schedulers.setFactory(Factory)`

Finally, there is a last customizable hook in `Schedulers`: `onHandleError`. This hook is
invoked whenever a `Runnable` task submitted to a `Scheduler` throws an `Exception` (note
that if there is an `UncaughtExceptionHandler` set for the `Thread` that ran the task,
both the handler and the hook will be invoked).


[[hooks]]
== Global Hooks
Reactor has another category of configurable callbacks that are invoked by Reactor
operators in various situations. They are all set in the `Hooks` class, and fall into
three categories:

[[hooks-dropping]]
=== Dropping Hooks
These hooks are invoked when the source of an operator doesn't comply to the Reactive
Streams specification. These kind of errors are outside of the normal execution path (ie
they can't be propagated through `onError`).

Typically, a `Publisher` calls `onNext` on the operator despite having already called
`onCompleted` on it previously. +
In that case, the `onNext` value is _dropped_. Same goes for an extraneous `onError`
signal.

The corresponding hooks, `onNextDropped` and `onErrorDropped`, allow you to provide a
global `Consumer` for these drops. You can for example use it to log the drop and cleanup
resources associated with a value if needed (as it will never make it to the rest of
the reactive chain).

Setting the hooks twice in a row is additive: every consumer you provide is invoked.
The hooks can be fully reset to default using `Hooks.resetOn*Dropped()` methods.

[[hooks-internal]]
=== Internal Error Hook
There is one such hook, `onOperatorError`, which is invoked by operators when an unexpected
`Exception` is thrown during the execution of their `onNext`, `onError` and `onComplete`
methods.

Unlike the previous category, this is still within the normal execution path. A typical
example is the `map` operator with a map function that throws an `Exception` (division by
zero for instance). It is still possible at this point to go through the usual channel of
`onError`, and that is what the operator does.

But first, it passes the `Exception` through `onOperatorError`. The hook lets you inspect
the error (and the incriminating value if relevant) and _change_ the `Exception`. Of course
you can also do something on the side like logging and return the original Exception.

Note that the `onOperatorError` hook can be set multiple times: you can provide a `String`
identifier for a particular `BiFunction`, and subsequent calls with different keys
concatenates the functions, which are all executed. On the other hand, reusing the same key
twice allow you to replace a function you previously set.

As a consequence, the default hook behavior can be both fully reset (using
`Hooks.resetOnOperatorError()`) or partially reset for a specific `key` only
(using `Hooks.resetOnOperatorError(String)`).

[[hooks-assembly]]
=== Assembly Hooks
These hooks tie in the lifecycle of operators. They are invoked when a chain of operators
is assembled (ie. instantiated). `onEachOperator` allow you to dynamically change each
operator as it is assembled in the chain, by returning a different `Publisher`.
`onLastOperator` is similar, except it is only invoked on the last operator in the chain
before the `subscribe`.

Like `onOperatorError`, these hooks are cumulative and can be identified with a key. As
such they can be reset partially or totally.

=== Hook Presets
The `Hooks` utility class provides a couple of preset hooks. These are alternatives to the
default behaviors that you can use simply by calling their corresponding method, rather
than coming up with the hook yourself:

 - `onNextDroppedFail()`: `onNextDropped` used to throw a `Exceptions.failWithCancel()`
 exception. It now defaults to logging the dropped value at DEBUG level. To go back to the
 old default behavior of throwing, use `onNextDroppedFail()`.

 - `onOperatorDebug()`: this activates <<debug-activate,debug mode>>. It ties into the
 `onOperatorError` hook, so calling `resetOnOperatorError()` will also reset it. It can be
 independently reset via `resetOnOperatorDebug()` as it uses a specific key internally.