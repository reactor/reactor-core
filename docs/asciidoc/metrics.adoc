[[metrics]]
= Exposing Reactor metrics

Project Reactor is a library designed for performance and better utilization of resources.
But to truly understand the performance of a system, it is best to be able to monitor its various components.

This is why Reactor provides a built-in integration with https://micrometer.io[Micrometer].

TIP: If Micrometer is not on the classpath, metrics will be a no-op.

== Scheduler metrics

Every async operation in Reactor is done via the Scheduler abstraction described in <<schedulers>>.
This is why it is important to monitor your schedulers, watch out for key metrics that start to look suspicious and react accordingly.

To enable scheduler metrics, you will need to use the following method:
====
[source,java]
----
Schedulers.enableMetrics();
----
====

WARNING: The instrumentation is performed when a scheduler is created. It is recommended to call this method as early as possible.

TIP: If you're using Spring Boot, it is a good idea to place the invocation before `SpringApplication.run(Application.class, args)` call.

Once scheduler metrics are enabled and provided it is on the classpath, Reactor will use Micrometer's support for instrumenting the executors that back most schedulers.

Please refer to https://micrometer.io/docs/ref/jvm[Micrometer's documentation] for the exposed metrics, such as:

- executor_active_threads
- executor_completed_tasks_total
- executor_pool_size_threads
- executor_queued_tasks
- executor_secounds_{count, max, sum}

Since one scheduler may have multiple executors, every executor metric has a `reactor_scheduler_id` tag.

TIP: Grafana + Prometheus users can use https://raw.githubusercontent.com/reactor/reactor-monitoring-demo/master/dashboards/schedulers.json[a pre-built dashboard] which includes panels for threads, completed tasks, task queues and other handy metrics.

== Publisher metrics
Sometimes it is useful to be able to record metrics at some stage in your reactive pipeline.

One way to do it would be to manually push the values to your metrics backend of choice.
Another option would be to use Reactor's built-in metrics integration for `Flux`/`Mono` and interpret them.

Consider the following pipeline:
====
[source,java]
----
listenToEvents()
    .doOnNext(event -> log.info("Received {}", event))
    .delayUntil(this::processEvent)
    .retry()
    .subscribe();
----
====

To enable the metrics for this source `Flux` (returned from `listenToEvents()`), we need to turn on the metrics collecting:

====
[source,java]
----
listenToEvents()
    .name("events") <1>
    .metrics() <2>
    .doOnNext(event -> log.info("Received {}", event))
    .delayUntil(this::processEvent)
    .retry()
    .subscribe();
----
<1> Every metric at this stage will be identified as "events" (optional).
<2> `Flux#metrics` operator enables the reporting of metrics, using the name provided in when calling `Flux#name` operator. In case `Flux#name` operator has not been used, the default name will be `reactor`.
====

Just adding these two operators will expose a whole bunch of useful metrics!

[width="100%",options="header"]
|=======
| metric name | type | description

| [name].subscribed | Counter | Counts how many Reactor sequences have been subscribed to

| [name].malformed.source | Counter | Counts the number of events received from a malformed source (ie an onNext after an onComplete)

| [name].requested | DistributionSummary | Counts the amount requested to a named Flux by all subscribers, until at least one requests an unbounded amount

| [name].onNext.delay | Timer | Measures delays between onNext signals (or between onSubscribe and first onNext)

| [name].flow.duration | Timer | Times the duration elapsed between a subscription and the termination or cancellation of the sequence. A status tag is added to specify what event caused the timer to end (`completed`, `completedEmpty`, `error`, `cancelled`).
|=======

Want to know how many times your event processing has restarted due to some error? Read `[name].subscribed`, because `retry()` operator will re-subscribe to the source publisher on error.

Interested in "events per second" metric? Measure the rate of `[name].onNext.delay` 's count.

Want to be alerted when the listener throws an error? `[name].flow.duration` with `status=error` tag is your friend.
Similarly, `status=completed` and `status=completedEmpty` will allow you to distinguish sequences that completed with elements from sequences that completed empty.

Please note that when giving a name to a sequence, this sequence could not be aggregated with others anymore. As a compromise if you want to identify your sequence but still make it possible to aggregate with other views, you can use a <<Tags>> for the name by calling `(tag("flow", "events"))` for example.

=== Tags

Every metric will have a `type` tag in common, which value will be either `Flux` or `Mono` depending on the publisher's nature.

Users are allowed to add custom tags to their reactive chains:
====
[source,java]
----
listenToEvents()
    .name("events") <1>
    .tag("source", "kafka") <2>
    .metrics() <3>
    .doOnNext(event -> log.info("Received {}", event))
    .delayUntil(this::processEvent)
    .retry()
    .subscribe();
----
<1> Every metric at this stage will be identified as "events".
<2> Set a custom tag "source" to value "kafka".
<3> All reported metrics will have `source=kafka` tag assigned in addition to the common tag described above.
====

Please note that depending on the monitoring system you're using, using a name can be considered mandatory when using tags, since it would otherwise result in a different set of tags between two default-named sequences.
Some systems like Prometheus might also require to have the exact same set of tags for each metric with the same name.
